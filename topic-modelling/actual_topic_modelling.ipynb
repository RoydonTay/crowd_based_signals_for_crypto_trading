{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeAQiCWIRNFK"
   },
   "source": [
    "actual topic modelling: finding actual topics and aggregating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPFAodiLRY7e"
   },
   "source": [
    "**setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4666,
     "status": "ok",
     "timestamp": 1758451631530,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "fEsAh_zpRVcz",
    "outputId": "bd93fb8b-2f41-430d-fccb-a24f6d228d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1758451631540,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "M_uOKb4sRYG2",
    "outputId": "33f6359d-21ed-4932-ad08-d459236b7c91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/topic-modelling\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/topic-modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thnK5JuERfDG"
   },
   "source": [
    "installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4923,
     "status": "ok",
     "timestamp": 1758451636463,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "jt3SZ2a1Redp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1758451636662,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "0_TeqC0VRuj9",
    "outputId": "1cb3e303-56a4-4d76-8830-7b6d7c84cd74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HZhU5bLR-bm"
   },
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1428,
     "status": "ok",
     "timestamp": 1758451638100,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "kpcHaZPJR_ur"
   },
   "outputs": [],
   "source": [
    "eth_df = pd.read_csv(\"extracted_datasets/eth_reddit_data.csv\")\n",
    "btc_df = pd.read_csv(\"extracted_datasets/btc_reddit_data.csv\")\n",
    "cryptomarkets_df = pd.read_csv(\"extracted_datasets/cryptomarkets_reddit_data.csv\")\n",
    "combined_df = pd.read_csv(\"extracted_datasets/combined_reddit_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1758451638155,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "RgwsVqh2RLOX"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RedditDataPreprocessor:\n",
    "    def __init__(self, min_score=-5, min_text_length=20, max_text_length=5000, min_words=5, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor with filtering parameters\n",
    "\n",
    "        Parameters:\n",
    "        - min_score: Minimum post score to keep (default: -5)\n",
    "        - min_text_length: Minimum character length for combined text\n",
    "        - max_text_length: Maximum character length for combined text\n",
    "        - min_words: Minimum words after preprocessing\n",
    "        - random_state: Random seed for sampling\n",
    "        \"\"\"\n",
    "        self.min_score = min_score\n",
    "        self.min_text_length = min_text_length\n",
    "        self.max_text_length = max_text_length\n",
    "        self.min_words = min_words\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # initialize NLTK components\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # add crypto-specific stop words\n",
    "        crypto_stopwords = {\n",
    "            'crypto', 'cryptocurrency', 'btc', 'bitcoin', 'eth', 'ethereum',\n",
    "            'coin', 'token', 'blockchain', 'would', 'could', 'one', 'get',\n",
    "            'like', 'think', 'know', 'people', 'time', 'good', 'make',\n",
    "            'price', 'market', 'trading', 'buy', 'sell', 'hold'\n",
    "        }\n",
    "        self.stop_words.update(crypto_stopwords)\n",
    "\n",
    "    def is_crypto_relevant(self, text):\n",
    "        \"\"\"Check if text contains crypto/finance relevant content\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return False\n",
    "\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        # chatgpted crypto-relevant keywords\n",
    "        crypto_keywords = {\n",
    "            # core crypto terms\n",
    "            'crypto', 'bitcoin', 'btc', 'ethereum', 'eth', 'blockchain', 'mining',\n",
    "            'wallet', 'exchange', 'trading', 'price', 'market', 'investment',\n",
    "            'portfolio', 'hodl', 'bull', 'bear', 'moon', 'dump', 'pump',\n",
    "\n",
    "            # technical terms\n",
    "            'defi', 'nft', 'token', 'coin', 'satoshi', 'hash', 'node',\n",
    "            'altcoin', 'staking', 'yield', 'liquidity', 'dex', 'centralized',\n",
    "            'decentralized', 'smart contract', 'gas fee', 'transaction',\n",
    "\n",
    "            # market/regulatory terms\n",
    "            'regulation', 'sec', 'institutional', 'adoption', 'volatility',\n",
    "            'futures', 'options', 'leverage', 'margin', 'short', 'long',\n",
    "\n",
    "            # popular coins/projects\n",
    "            'doge', 'ada', 'sol', 'matic', 'link', 'dot', 'uni', 'aave',\n",
    "            'chainlink', 'polygon', 'solana', 'cardano', 'binance', 'coinbase'\n",
    "        }\n",
    "\n",
    "        return any(keyword in text_lower for keyword in crypto_keywords)\n",
    "\n",
    "    def is_spam_or_irrelevant(self, title, selftext):\n",
    "        \"\"\"Detect spam, promotional, or low-quality content\"\"\"\n",
    "        combined_text = f\"{title} {selftext}\".lower() if not pd.isna(title) and not pd.isna(selftext) else \"\"\n",
    "\n",
    "        # spam/promotional patterns\n",
    "        spam_patterns = [\n",
    "            r'check out my.*channel',\n",
    "            r'subscribe.*youtube',\n",
    "            r'follow me on',\n",
    "            r'dm me',\n",
    "            r'private message',\n",
    "            r'\\$\\d+.*guaranteed',\n",
    "            r'100% profit',\n",
    "            r'risk free',\n",
    "            r'click here',\n",
    "            r'limited time',\n",
    "            r'act now',\n",
    "            r'free money',\n",
    "            r'telegram.*group',\n",
    "            r'discord.*server',\n",
    "            r'referral.*link',\n",
    "            r'use my code'\n",
    "        ]\n",
    "\n",
    "        for pattern in spam_patterns:\n",
    "            if re.search(pattern, combined_text):\n",
    "                return True\n",
    "\n",
    "        # check for excessive links (likely spam)\n",
    "        if combined_text.count('http') > 2:\n",
    "            return True\n",
    "\n",
    "        # check for very short, low-effort posts\n",
    "        if len(combined_text.strip()) < 10:\n",
    "            return True\n",
    "\n",
    "        # check for posts that are just emojis or special characters\n",
    "        if re.sub(r'[^a-zA-Z0-9\\s]', '', combined_text).strip() == '':\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess text for topic modeling\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        # convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "        # remove Reddit-specific formatting\n",
    "        text = re.sub(r'/u/\\S+', '', text)  # Remove user mentions\n",
    "        text = re.sub(r'/r/\\S+', '', text)  # Remove subreddit mentions\n",
    "        text = re.sub(r'\\[deleted\\]|\\[removed\\]', '', text)\n",
    "\n",
    "        # remove special characters but keep some punctuation\n",
    "        text = re.sub(r'[^a-zA-Z\\s\\.\\!\\?]', ' ', text)\n",
    "\n",
    "        # remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # remove stopwords, lemmatize, and filter short words\n",
    "        tokens = [\n",
    "            self.lemmatizer.lemmatize(token)\n",
    "            for token in tokens\n",
    "            if token not in self.stop_words and len(token) > 2 and token.isalpha()\n",
    "        ]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract date from filename pattern\"\"\"\n",
    "        if pd.isna(filename):\n",
    "            return None\n",
    "\n",
    "        # pattern: reddit_data_eth_2025-08-07.csv\n",
    "        date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', filename)\n",
    "        if date_match:\n",
    "            return pd.to_datetime(date_match.group(1))\n",
    "        return None\n",
    "\n",
    "    def filter_relevant_posts(self, df):\n",
    "        \"\"\"Apply comprehensive filtering to remove irrelevant/low-quality posts\"\"\"\n",
    "        print(f\"Initial dataset size: {len(df):,}\")\n",
    "\n",
    "        # create a copy to avoid modifying original\n",
    "        df_filtered = df.copy()\n",
    "\n",
    "        # remove posts with missing essential data\n",
    "        initial_size = len(df_filtered)\n",
    "        df_filtered = df_filtered.dropna(subset=['score'])\n",
    "        if len(df_filtered) < initial_size:\n",
    "            print(f\"Removed {initial_size - len(df_filtered)} posts with missing scores\")\n",
    "\n",
    "        # remove posts with very low scores (heavily downvoted)\n",
    "        df_filtered = df_filtered[df_filtered['score'] >= self.min_score]\n",
    "        print(f\"After score filter (>= {self.min_score}): {len(df_filtered):,}\")\n",
    "\n",
    "        # combine title and selftext\n",
    "        df_filtered['combined_text'] = (\n",
    "            df_filtered['title'].fillna('') + ' ' + df_filtered['selftext'].fillna('')\n",
    "        ).str.strip()\n",
    "\n",
    "        # remove posts that are too short or too long\n",
    "        text_lengths = df_filtered['combined_text'].str.len()\n",
    "        df_filtered = df_filtered[\n",
    "            (text_lengths >= self.min_text_length) &\n",
    "            (text_lengths <= self.max_text_length)\n",
    "        ]\n",
    "        print(f\"After text length filter ({self.min_text_length}-{self.max_text_length} chars): {len(df_filtered):,}\")\n",
    "\n",
    "        # remove deleted/removed posts\n",
    "        deleted_mask = (\n",
    "            (df_filtered['title'].isin(['[deleted]', '[removed]'])) |\n",
    "            (df_filtered['selftext'].isin(['[deleted]', '[removed]'])) |\n",
    "            (df_filtered['combined_text'].str.contains(r'\\[deleted\\]|\\[removed\\]', na=False))\n",
    "        )\n",
    "        df_filtered = df_filtered[~deleted_mask]\n",
    "        print(f\"After removing deleted posts: {len(df_filtered):,}\")\n",
    "\n",
    "        # apply relevance and spam filtering\n",
    "        print(\"Filtering for crypto relevance and removing spam...\")\n",
    "        relevant_mask = df_filtered.apply(\n",
    "            lambda row: (\n",
    "                self.is_crypto_relevant(row['combined_text']) and\n",
    "                not self.is_spam_or_irrelevant(row['title'], row['selftext'])\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        df_filtered = df_filtered[relevant_mask]\n",
    "        print(f\"After relevance and spam filter: {len(df_filtered):,}\")\n",
    "\n",
    "        return df_filtered\n",
    "\n",
    "    def smart_sampling(self, df, max_posts=25000):\n",
    "        \"\"\"Apply smart sampling for large datasets\"\"\"\n",
    "        if len(df) <= max_posts:\n",
    "            return df\n",
    "\n",
    "        print(f\"Dataset large ({len(df):,} posts), applying engagement-based sampling...\")\n",
    "\n",
    "        # define high engagement threshold (top 30%)\n",
    "        engagement_threshold = df['score'].quantile(0.7)\n",
    "\n",
    "        high_engagement = df[df['score'] >= engagement_threshold]\n",
    "        low_engagement = df[df['score'] < engagement_threshold]\n",
    "\n",
    "        # keep all high engagement posts, sample from low engagement\n",
    "        remaining_slots = max_posts - len(high_engagement)\n",
    "\n",
    "        if remaining_slots > 0 and len(low_engagement) > 0:\n",
    "            sample_size = min(remaining_slots, len(low_engagement))\n",
    "            low_engagement_sample = low_engagement.sample(\n",
    "                n=sample_size,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "\n",
    "            df_sampled = pd.concat([high_engagement, low_engagement_sample], ignore_index=True)\n",
    "        else:\n",
    "            # if too many high engagement posts, sample from those too\n",
    "            df_sampled = high_engagement.sample(n=max_posts, random_state=self.random_state)\n",
    "\n",
    "        print(f\"After smart sampling: {len(df_sampled):,}\")\n",
    "        print(f\"  - High engagement posts: {len(high_engagement):,}\")\n",
    "        if 'low_engagement_sample' in locals():\n",
    "            print(f\"  - Low engagement sample: {len(low_engagement_sample):,}\")\n",
    "\n",
    "        return df_sampled\n",
    "\n",
    "    def preprocess_dataset(self, df, apply_sampling=True, max_posts=25000):\n",
    "        \"\"\"Complete preprocessing pipeline for a dataset\"\"\"\n",
    "\n",
    "        # apply filtering\n",
    "        df_filtered = self.filter_relevant_posts(df)\n",
    "\n",
    "        if len(df_filtered) == 0:\n",
    "            print(\"Warning: No posts remaining after filtering!\")\n",
    "            return df_filtered\n",
    "\n",
    "        # apply smart sampling if requested and dataset is large\n",
    "        if apply_sampling:\n",
    "            df_filtered = self.smart_sampling(df_filtered, max_posts)\n",
    "\n",
    "        # extract dates from filename\n",
    "        df_filtered['date'] = df_filtered['filename'].apply(self.extract_date_from_filename)\n",
    "\n",
    "        # preprocess text\n",
    "        print(\"Preprocessing text...\")\n",
    "        df_filtered['processed_text'] = df_filtered['combined_text'].apply(self.preprocess_text)\n",
    "\n",
    "        # final filter: remove texts that are too short after preprocessing\n",
    "        word_counts = df_filtered['processed_text'].apply(lambda x: len(x.split()) if x else 0)\n",
    "        initial_size = len(df_filtered)\n",
    "        df_filtered = df_filtered[word_counts >= self.min_words]\n",
    "\n",
    "        if len(df_filtered) < initial_size:\n",
    "            print(f\"After preprocessing filter (>= {self.min_words} words): {len(df_filtered):,}\")\n",
    "\n",
    "        return df_filtered\n",
    "\n",
    "    def save_preprocessed_data(self, df, filename):\n",
    "        \"\"\"Save preprocessed data to CSV\"\"\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved preprocessed data to: {filename}\")\n",
    "\n",
    "def preprocess_all_datasets():\n",
    "    \"\"\"Main function to preprocess all crypto datasets\"\"\"\n",
    "\n",
    "    # initialize preprocessor\n",
    "    preprocessor = RedditDataPreprocessor(\n",
    "        min_score=-5,\n",
    "        min_text_length=20,\n",
    "        max_text_length=5000,\n",
    "        min_words=5,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # load datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    try:\n",
    "        eth_df = pd.read_csv('extracted_datasets/eth_reddit_data.csv')\n",
    "        btc_df = pd.read_csv('extracted_datasets/btc_reddit_data.csv')\n",
    "        cryptomarkets_df = pd.read_csv('extracted_datasets/cryptomarkets_reddit_data.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading datasets: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Raw dataset sizes:\")\n",
    "    print(f\"  ETH: {len(eth_df):,}\")\n",
    "    print(f\"  BTC: {len(btc_df):,}\")\n",
    "    print(f\"  CryptoMarkets: {len(cryptomarkets_df):,}\")\n",
    "    print(f\"  Total: {len(eth_df) + len(btc_df) + len(cryptomarkets_df):,}\")\n",
    "\n",
    "    # preprocess each dataset\n",
    "    results = {}\n",
    "\n",
    "    for name, df in [('eth', eth_df), ('btc', btc_df), ('cryptomarkets', cryptomarkets_df)]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PREPROCESSING {name.upper()} DATASET\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        preprocessed_df = preprocessor.preprocess_dataset(df, apply_sampling=True)\n",
    "\n",
    "        if len(preprocessed_df) > 0:\n",
    "\n",
    "            results[name] = preprocessed_df\n",
    "        else:\n",
    "            print(f\"No data remaining for {name} after preprocessing\")\n",
    "            results[name] = pd.DataFrame()\n",
    "\n",
    "    # create combined preprocessed dataset\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CREATING COMBINED PREPROCESSED DATASET\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    combined_preprocessed = pd.concat([\n",
    "        df for df in results.values() if len(df) > 0\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    if len(combined_preprocessed) > 0:\n",
    "        # apply smart sampling to combined dataset if it's too large\n",
    "        if len(combined_preprocessed) > 30000:\n",
    "            combined_preprocessed = preprocessor.smart_sampling(combined_preprocessed, max_posts=30000)\n",
    "\n",
    "        results['combined'] = combined_preprocessed\n",
    "\n",
    "    # print summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PREPROCESSING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    total_original = len(eth_df) + len(btc_df) + len(cryptomarkets_df)\n",
    "    total_processed = sum(len(df) for df in results.values() if len(df) > 0)\n",
    "\n",
    "    print(f\"Total original posts: {total_original:,}\")\n",
    "    print(f\"Total processed posts: {total_processed:,}\")\n",
    "    print(f\"Overall reduction: {((total_original - total_processed) / total_original * 100):.1f}%\")\n",
    "\n",
    "    print(f\"\\nDataset breakdown:\")\n",
    "    for name, df in results.items():\n",
    "        if len(df) > 0:\n",
    "            print(f\"  {name}: {len(df):,} posts\")\n",
    "            if 'date' in df.columns:\n",
    "                date_range = f\"{df['date'].min()} to {df['date'].max()}\"\n",
    "                print(f\"    Date range: {date_range}\")\n",
    "            if 'subreddit' in df.columns:\n",
    "                print(f\"    Subreddits: {df['subreddit'].nunique()}\")\n",
    "\n",
    "    return results, preprocessor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64105,
     "status": "ok",
     "timestamp": 1758451702262,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "MSNZg_enSHzr",
    "outputId": "5627fb4e-978b-4bd7-f75e-fffbcc42fab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Raw dataset sizes:\n",
      "  ETH: 24,330\n",
      "  BTC: 25,168\n",
      "  CryptoMarkets: 13,269\n",
      "  Total: 62,767\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING ETH DATASET\n",
      "============================================================\n",
      "Initial dataset size: 24,330\n",
      "After score filter (>= -5): 24,257\n",
      "After text length filter (20-5000 chars): 24,016\n",
      "After removing deleted posts: 23,876\n",
      "Filtering for crypto relevance and removing spam...\n",
      "After relevance and spam filter: 20,802\n",
      "Preprocessing text...\n",
      "After preprocessing filter (>= 5 words): 16,745\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING BTC DATASET\n",
      "============================================================\n",
      "Initial dataset size: 25,168\n",
      "After score filter (>= -5): 24,993\n",
      "After text length filter (20-5000 chars): 24,915\n",
      "After removing deleted posts: 24,747\n",
      "Filtering for crypto relevance and removing spam...\n",
      "After relevance and spam filter: 22,276\n",
      "Preprocessing text...\n",
      "After preprocessing filter (>= 5 words): 20,658\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING CRYPTOMARKETS DATASET\n",
      "============================================================\n",
      "Initial dataset size: 13,269\n",
      "After score filter (>= -5): 13,115\n",
      "After text length filter (20-5000 chars): 13,055\n",
      "After removing deleted posts: 12,972\n",
      "Filtering for crypto relevance and removing spam...\n",
      "After relevance and spam filter: 12,252\n",
      "Preprocessing text...\n",
      "After preprocessing filter (>= 5 words): 9,034\n",
      "\n",
      "============================================================\n",
      "CREATING COMBINED PREPROCESSED DATASET\n",
      "============================================================\n",
      "Dataset large (46,437 posts), applying engagement-based sampling...\n",
      "After smart sampling: 30,000\n",
      "  - High engagement posts: 18,871\n",
      "  - Low engagement sample: 11,129\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING SUMMARY\n",
      "============================================================\n",
      "Total original posts: 62,767\n",
      "Total processed posts: 76,437\n",
      "Overall reduction: -21.8%\n",
      "\n",
      "Dataset breakdown:\n",
      "  eth: 16,745 posts\n",
      "    Date range: 2025-08-06 00:00:00 to 2025-09-12 00:00:00\n",
      "    Subreddits: 1\n",
      "  btc: 20,658 posts\n",
      "    Date range: 2025-08-06 00:00:00 to 2025-09-12 00:00:00\n",
      "    Subreddits: 1\n",
      "  cryptomarkets: 9,034 posts\n",
      "    Date range: 2025-08-06 00:00:00 to 2025-08-13 00:00:00\n",
      "    Subreddits: 1\n",
      "  combined: 30,000 posts\n",
      "    Date range: 2025-08-06 00:00:00 to 2025-09-12 00:00:00\n",
      "    Subreddits: 3\n"
     ]
    }
   ],
   "source": [
    "results, preprocessor = preprocess_all_datasets()\n",
    "preprocessed_datasets = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1758451702358,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "7IzuxpuuTIiz"
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class LDATopicModeler:\n",
    "    def __init__(self, n_topics=10, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the topic modeler\n",
    "\n",
    "        Parameters:\n",
    "        - n_topics: Number of topics to discover (default: 10)\n",
    "        - random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "        self.random_state = random_state\n",
    "        self.lda_model = None\n",
    "        self.vectorizer = None\n",
    "        self.feature_names = None\n",
    "        self.topic_names = None\n",
    "\n",
    "    def fit_lda_model(self, texts, optimize_topics=True, max_features=1000):\n",
    "        \"\"\"\n",
    "        Fit LDA model on preprocessed texts\n",
    "\n",
    "        Parameters:\n",
    "        - texts: List of preprocessed text documents\n",
    "        - optimize_topics: Whether to optimize number of topics via grid search\n",
    "        - max_features: Maximum number of features for TF-IDF vectorizer\n",
    "        \"\"\"\n",
    "        print(f\"Fitting LDA model on {len(texts):,} documents...\")\n",
    "\n",
    "        # Create TF-IDF vectorizer\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "            max_df=0.8,  # Ignore terms that appear in more than 80% of documents\n",
    "            ngram_range=(1, 2),  # Use both unigrams and bigrams\n",
    "            stop_words='english'\n",
    "        )\n",
    "\n",
    "        # Fit and transform texts\n",
    "        print(\"Creating document-term matrix...\")\n",
    "        doc_term_matrix = self.vectorizer.fit_transform(texts)\n",
    "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "\n",
    "        print(f\"Document-term matrix shape: {doc_term_matrix.shape}\")\n",
    "\n",
    "        # optimize number of topics if requested\n",
    "        if optimize_topics and len(texts) > 100:\n",
    "            print(\"Optimizing number of topics...\")\n",
    "            self.n_topics = self._optimize_topic_number(doc_term_matrix)\n",
    "\n",
    "        # fit final LDA model\n",
    "        print(f\"Fitting LDA with {self.n_topics} topics...\")\n",
    "        self.lda_model = LatentDirichletAllocation(\n",
    "            n_components=self.n_topics,\n",
    "            random_state=self.random_state,\n",
    "            max_iter=20,\n",
    "            learning_method='batch',\n",
    "            doc_topic_prior=None,  # use default symmetric prior\n",
    "            topic_word_prior=None  # use default symmetric prior\n",
    "        )\n",
    "\n",
    "        self.lda_model.fit(doc_term_matrix)\n",
    "\n",
    "        # generate topic names\n",
    "        self.topic_names = self._generate_topic_names()\n",
    "\n",
    "        print(\"LDA model fitting complete!\")\n",
    "        return doc_term_matrix\n",
    "\n",
    "    def _optimize_topic_number(self, doc_term_matrix):\n",
    "        \"\"\"Optimize number of topics using grid search with perplexity\"\"\"\n",
    "        topic_range = [5, 8, 10, 12, 15]\n",
    "\n",
    "        # for very large datasets, limit topic range\n",
    "        if doc_term_matrix.shape[0] > 20000:\n",
    "            topic_range = [5, 8, 10]\n",
    "\n",
    "        print(f\"Testing topic numbers: {topic_range}\")\n",
    "\n",
    "        param_grid = {'n_components': topic_range}\n",
    "        lda = LatentDirichletAllocation(\n",
    "            random_state=self.random_state,\n",
    "            max_iter=10,\n",
    "            learning_method='batch'\n",
    "        )\n",
    "\n",
    "        # use 3-fold CV, but limit to avoid overfitting on small datasets\n",
    "        cv_folds = min(3, max(2, len(topic_range)))\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            lda,\n",
    "            param_grid,\n",
    "            cv=cv_folds,\n",
    "            scoring='neg_mean_squared_error',  # use MSE as proxy for perplexity\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        grid_search.fit(doc_term_matrix)\n",
    "        optimal_topics = grid_search.best_params_['n_components']\n",
    "\n",
    "        print(f\"Optimal number of topics: {optimal_topics}\")\n",
    "        return optimal_topics\n",
    "\n",
    "    def _generate_topic_names(self, n_words=5):\n",
    "        \"\"\"Generate interpretable topic names based on top words\"\"\"\n",
    "        if not self.lda_model or self.feature_names is None:\n",
    "            return []\n",
    "\n",
    "        topic_names = []\n",
    "\n",
    "        for topic_idx, topic in enumerate(self.lda_model.components_):\n",
    "            # get top words for this topic\n",
    "            top_word_indices = topic.argsort()[-n_words:][::-1]\n",
    "            top_words = [self.feature_names[i] for i in top_word_indices]\n",
    "\n",
    "            # create descriptive name\n",
    "            topic_name = f\"Topic_{topic_idx}: {', '.join(top_words[:3])}\"\n",
    "            topic_names.append(topic_name)\n",
    "\n",
    "        return topic_names\n",
    "\n",
    "    def get_topic_details(self, n_words=10):\n",
    "        \"\"\"Get detailed information about each topic\"\"\"\n",
    "        if not self.lda_model or self.feature_names is None:\n",
    "            return []\n",
    "\n",
    "        topic_details = []\n",
    "\n",
    "        for topic_idx, topic in enumerate(self.lda_model.components_):\n",
    "            # get top words with their weights\n",
    "            top_word_indices = topic.argsort()[-n_words:][::-1]\n",
    "            top_words_weights = [\n",
    "                (self.feature_names[i], topic[i])\n",
    "                for i in top_word_indices\n",
    "            ]\n",
    "\n",
    "            topic_details.append({\n",
    "                'topic_id': topic_idx,\n",
    "                'topic_name': self.topic_names[topic_idx] if self.topic_names else f\"Topic_{topic_idx}\",\n",
    "                'top_words': top_words_weights\n",
    "            })\n",
    "\n",
    "        return topic_details\n",
    "\n",
    "    def get_document_topics(self, texts):\n",
    "        \"\"\"Get topic probabilities for documents\"\"\"\n",
    "        if not self.lda_model or not self.vectorizer:\n",
    "            raise ValueError(\"Model not fitted. Call fit_lda_model first.\")\n",
    "\n",
    "        # transform texts using the fitted vectorizer\n",
    "        doc_term_matrix = self.vectorizer.transform(texts)\n",
    "\n",
    "        # get topic probabilities\n",
    "        doc_topic_probs = self.lda_model.transform(doc_term_matrix)\n",
    "\n",
    "        return doc_topic_probs\n",
    "\n",
    "    def create_daily_topic_features(self, df):\n",
    "        \"\"\"\n",
    "        Create daily aggregated topic features from preprocessed dataframe\n",
    "\n",
    "        Parameters:\n",
    "        - df: Preprocessed dataframe with 'processed_text', 'date', 'subreddit', 'score' columns\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame with daily topic features\n",
    "        \"\"\"\n",
    "        print(\"Creating daily topic features...\")\n",
    "\n",
    "        if 'processed_text' not in df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'processed_text' column\")\n",
    "\n",
    "        # get topic probabilities for all documents\n",
    "        topic_probs = self.get_document_topics(df['processed_text'])\n",
    "\n",
    "        # add topic probabilities to dataframe\n",
    "        topic_columns = [f'topic_{i}' for i in range(self.n_topics)]\n",
    "        for i, col in enumerate(topic_columns):\n",
    "            df[col] = topic_probs[:, i]\n",
    "\n",
    "        # add dominant topic\n",
    "        df['dominant_topic'] = topic_probs.argmax(axis=1)\n",
    "\n",
    "        # calculate topic confidence (max probability)\n",
    "        df['topic_confidence'] = topic_probs.max(axis=1)\n",
    "\n",
    "        # group by date and subreddit for daily aggregation\n",
    "        daily_features_list = []\n",
    "\n",
    "        groupby_cols = ['date', 'subreddit'] if 'subreddit' in df.columns else ['date']\n",
    "\n",
    "        for group_keys, group_df in df.groupby(groupby_cols):\n",
    "            if isinstance(group_keys, tuple):\n",
    "                date, subreddit = group_keys\n",
    "            else:\n",
    "                date, subreddit = group_keys, 'unknown'\n",
    "\n",
    "            if pd.isna(date):\n",
    "                continue\n",
    "\n",
    "            # basic features\n",
    "            features = {\n",
    "                'date': date,\n",
    "                'subreddit': subreddit,\n",
    "                'post_count': len(group_df),\n",
    "                'avg_score': group_df['score'].mean(),\n",
    "                'total_score': group_df['score'].sum(),\n",
    "                'score_std': group_df['score'].std() if len(group_df) > 1 else 0,\n",
    "                'avg_topic_confidence': group_df['topic_confidence'].mean(),\n",
    "            }\n",
    "\n",
    "            # topic probability features (daily averages and volatility)\n",
    "            for i, topic_col in enumerate(topic_columns):\n",
    "                features[f'avg_{topic_col}'] = group_df[topic_col].mean()\n",
    "                features[f'std_{topic_col}'] = group_df[topic_col].std() if len(group_df) > 1 else 0\n",
    "                features[f'max_{topic_col}'] = group_df[topic_col].max()\n",
    "\n",
    "            # dominant topic distribution\n",
    "            dominant_counts = group_df['dominant_topic'].value_counts()\n",
    "            for topic_idx in range(self.n_topics):\n",
    "                count = dominant_counts.get(topic_idx, 0)\n",
    "                features[f'dominant_topic_{topic_idx}_count'] = count\n",
    "                features[f'dominant_topic_{topic_idx}_pct'] = count / len(group_df)\n",
    "\n",
    "            # topic diversity metrics\n",
    "            # shannon entropy of topic distribution\n",
    "            topic_dist = group_df['dominant_topic'].value_counts(normalize=True)\n",
    "            if len(topic_dist) > 1:\n",
    "                entropy = -sum(p * np.log2(p) for p in topic_dist if p > 0)\n",
    "                features['topic_diversity_entropy'] = entropy\n",
    "            else:\n",
    "                features['topic_diversity_entropy'] = 0\n",
    "\n",
    "            # number of unique topics discussed\n",
    "            features['unique_topics_count'] = group_df['dominant_topic'].nunique()\n",
    "\n",
    "            # topic concentration (how focused the discussion is)\n",
    "            features['topic_concentration'] = dominant_counts.iloc[0] / len(group_df) if len(dominant_counts) > 0 else 0\n",
    "\n",
    "            daily_features_list.append(features)\n",
    "\n",
    "        # convert to DataFrame\n",
    "        daily_features_df = pd.DataFrame(daily_features_list)\n",
    "\n",
    "        # sort by date\n",
    "        daily_features_df = daily_features_df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "        print(f\"Created daily features: {daily_features_df.shape[0]} days, {daily_features_df.shape[1]} features\")\n",
    "\n",
    "        return daily_features_df\n",
    "\n",
    "def run_topic_modeling_pipeline(datasets):\n",
    "    \"\"\"\n",
    "    Main pipeline for topic modeling and daily feature creation\n",
    "\n",
    "    Parameters:\n",
    "    - datasets: Dictionary of preprocessed DataFrames {'eth': df, 'btc': df, etc.}\n",
    "    \"\"\"\n",
    "\n",
    "    # check if we have data\n",
    "    if not any(len(df) > 0 for df in datasets.values() if isinstance(df, pd.DataFrame)):\n",
    "        print(\"No preprocessed data found. Please check your datasets.\")\n",
    "        return None, None\n",
    "\n",
    "    # use combined dataset for training topics (if available), otherwise use largest dataset\n",
    "    if 'combined' in datasets and len(datasets['combined']) > 0:\n",
    "        training_data = datasets['combined']\n",
    "        print(f\"Using combined dataset for topic modeling: {len(training_data):,} posts\")\n",
    "    else:\n",
    "        # find the largest individual dataset\n",
    "        largest_name = max(\n",
    "            [k for k in datasets.keys() if k != 'combined'],\n",
    "            key=lambda x: len(datasets[x]) if isinstance(datasets[x], pd.DataFrame) else 0\n",
    "        )\n",
    "        training_data = datasets[largest_name]\n",
    "        print(f\"Using {largest_name} dataset for topic modeling: {len(training_data):,} posts\")\n",
    "\n",
    "    # initialize and fit topic model\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TOPIC MODELING\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    modeler = LDATopicModeler(n_topics=10, random_state=42)\n",
    "\n",
    "    # fit LDA model\n",
    "    modeler.fit_lda_model(\n",
    "        training_data['processed_text'].tolist(),\n",
    "        optimize_topics=True,\n",
    "        max_features=1000\n",
    "    )\n",
    "\n",
    "    # display discovered topics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DISCOVERED TOPICS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    topic_details = modeler.get_topic_details()\n",
    "    for topic in topic_details:\n",
    "        print(f\"\\n{topic['topic_name']}\")\n",
    "        top_words = [f\"{word}({weight:.3f})\" for word, weight in topic['top_words'][:5]]\n",
    "        print(f\"  Top words: {', '.join(top_words)}\")\n",
    "\n",
    "    # create daily features for each dataset\n",
    "    results = {}\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CREATING DAILY TOPIC FEATURES\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        if len(df) == 0:\n",
    "            print(f\"\\nSkipping {name}: no data\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {name.upper()} dataset...\")\n",
    "\n",
    "        try:\n",
    "            # Create daily topic features\n",
    "            daily_features = modeler.create_daily_topic_features(df)\n",
    "\n",
    "            # save results\n",
    "            output_file = f'actual_topic_modelled/{name}_daily_topic_features.csv'\n",
    "            daily_features.to_csv(output_file, index=False)\n",
    "            print(f\"Saved to: {output_file}\")\n",
    "\n",
    "            # store results\n",
    "            results[name] = daily_features\n",
    "\n",
    "            # print summary stats\n",
    "            if len(daily_features) > 0:\n",
    "                print(f\"  Shape: {daily_features.shape}\")\n",
    "                print(f\"  Date range: {daily_features['date'].min()} to {daily_features['date'].max()}\")\n",
    "                print(f\"  Avg posts per day: {daily_features['post_count'].mean():.1f}\")\n",
    "\n",
    "                # Show top topics by average probability\n",
    "                topic_cols = [col for col in daily_features.columns if col.startswith('avg_topic_')]\n",
    "                if topic_cols:\n",
    "                    avg_topic_probs = daily_features[topic_cols].mean().sort_values(ascending=False)\n",
    "                    print(f\"  Top topics: {', '.join(avg_topic_probs.head(3).index)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {name}: {e}\")\n",
    "            results[name] = pd.DataFrame()\n",
    "\n",
    "    # create summary report\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TOPIC MODELING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    print(f\"\\nModel Details:\")\n",
    "    print(f\"  Number of topics: {modeler.n_topics}\")\n",
    "    print(f\"  Vocabulary size: {len(modeler.feature_names) if modeler.feature_names is not None else 'N/A'}\")\n",
    "    print(f\"  Training documents: {len(training_data):,}\")\n",
    "\n",
    "    print(f\"\\nDaily Features Created:\")\n",
    "    total_days = 0\n",
    "    for name, df in results.items():\n",
    "        if len(df) > 0:\n",
    "            days = len(df)\n",
    "            total_days += days\n",
    "            print(f\"  {name}: {days} days of features\")\n",
    "        else:\n",
    "            print(f\"  {name}: No features created\")\n",
    "\n",
    "    print(f\"\\nTotal daily observations: {total_days}\")\n",
    "\n",
    "    # save topic model summary\n",
    "    topic_summary = pd.DataFrame([\n",
    "        {\n",
    "            'topic_id': topic['topic_id'],\n",
    "            'topic_name': topic['topic_name'],\n",
    "            'top_3_words': ', '.join([word for word, _ in topic['top_words'][:3]])\n",
    "        }\n",
    "        for topic in topic_details\n",
    "    ])\n",
    "\n",
    "    topic_summary.to_csv('actual_topic_modelled/lda_topic_summary.csv', index=False)\n",
    "    print(f\"\\nSaved topic summary to: actual_topic_modelled/lda_topic_summary.csv\")\n",
    "\n",
    "    return results, modeler\n",
    "\n",
    "\n",
    "\n",
    "# Alternative: BERTopic implementation (requires additional installation)\n",
    "def run_bertopic_modeling(texts, n_topics=10):\n",
    "    \"\"\"\n",
    "    Alternative topic modeling using BERTopic\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from bertopic import BERTopic\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        print(\"Running BERTopic modeling...\")\n",
    "\n",
    "        # initialize BERTopic model\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=\"all-MiniLM-L6-v2\",\n",
    "            nr_topics=n_topics,\n",
    "            verbose=True,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # fit model and get topics\n",
    "        topics, probs = topic_model.fit_transform(texts)\n",
    "\n",
    "        # get topic information\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "\n",
    "        print(f\"\\nBERTopic Results:\")\n",
    "        print(f\"Number of topics: {len(topic_info) - 1}\")  # -1 because of outlier topic\n",
    "        print(f\"Number of documents: {len(texts)}\")\n",
    "\n",
    "        print(f\"\\nTop Topics:\")\n",
    "        for idx, row in topic_info.head(10).iterrows():\n",
    "            if row['Topic'] != -1:  # Skip outlier topic\n",
    "                print(f\"  Topic {row['Topic']}: {row['Name']}\")\n",
    "\n",
    "        return topic_model, topics, probs, topic_info\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"BERTopic not installed. Run: pip install bertopic sentence-transformers\")\n",
    "        return None, None, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1758451702384,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "RgxFhK7RZXwj"
   },
   "outputs": [],
   "source": [
    "def analyze_topic_trends(daily_features_df, modeler, top_n_topics=5):\n",
    "    \"\"\"\n",
    "    Analyze topic trends over time - streamlined for forecasting insights\n",
    "\n",
    "    Parameters:\n",
    "    - daily_features_df: DataFrame with daily topic features\n",
    "    - modeler: Fitted CryptoTopicModeler\n",
    "    - top_n_topics: Number of top topics to analyze\n",
    "    \"\"\"\n",
    "    if len(daily_features_df) == 0:\n",
    "        print(\"No data to analyze\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"TOPIC TREND ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # get only numbered topic columns\n",
    "    topic_cols = []\n",
    "    for col in daily_features_df.columns:\n",
    "        if col.startswith('avg_topic_'):\n",
    "            try:\n",
    "                topic_num = int(col.split('_')[-1])\n",
    "                topic_cols.append(col)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    if not topic_cols:\n",
    "        print(\"No topic columns found\")\n",
    "        return\n",
    "\n",
    "    # calculate average topic probabilities\n",
    "    avg_topic_probs = daily_features_df[topic_cols].mean().sort_values(ascending=False)\n",
    "\n",
    "    print(f\"\\nTop {top_n_topics} Topics by Average Probability:\")\n",
    "    for i, (topic_col, avg_prob) in enumerate(avg_topic_probs.head(top_n_topics).items()):\n",
    "        topic_idx = int(topic_col.split('_')[-1])\n",
    "        topic_name = modeler.topic_names[topic_idx] if modeler.topic_names else f\"Topic {topic_idx}\"\n",
    "        print(f\"  {i+1}. {topic_name}: {avg_prob:.4f}\")\n",
    "\n",
    "    # show basic stats for forecasting context\n",
    "    if 'date' in daily_features_df.columns:\n",
    "        print(f\"\\nAnalysis covers: {daily_features_df['date'].min()} to {daily_features_df['date'].max()}\")\n",
    "        print(f\"Total days: {len(daily_features_df)}\")\n",
    "        if 'post_count' in daily_features_df.columns:\n",
    "            print(f\"Average posts per day: {daily_features_df['post_count'].mean():.1f}\")\n",
    "\n",
    "def create_forecasting_features(daily_features_df, lag_days=[1, 3]):\n",
    "    \"\"\"\n",
    "    Create minimal lagged features for time series forecasting\n",
    "\n",
    "    Parameters:\n",
    "    - daily_features_df: DataFrame with daily features\n",
    "    - lag_days: List of lag periods to create (reduced to just 1,3 days)\n",
    "    \"\"\"\n",
    "    if len(daily_features_df) == 0:\n",
    "        return daily_features_df\n",
    "\n",
    "    print(f\"Creating minimal forecasting features...\")\n",
    "\n",
    "    # sort by date\n",
    "    df = daily_features_df.sort_values('date').copy()\n",
    "\n",
    "    # only create features for key columns that matter for forecasting\n",
    "    key_features = [\n",
    "        'post_count',           # volume of discussion\n",
    "        'avg_score',            # sentiment proxy\n",
    "        'avg_topic_0',          # top topics only\n",
    "        'avg_topic_1',\n",
    "        'avg_topic_4',\n",
    "        'avg_topic_7'\n",
    "    ]\n",
    "\n",
    "    # filter to existing columns\n",
    "    key_features = [col for col in key_features if col in df.columns]\n",
    "\n",
    "    # create only short-term lags (1-3 days) for key features\n",
    "    for lag in lag_days:\n",
    "        for col in key_features:\n",
    "            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "\n",
    "    # create 3-day rolling average only for post_count and avg_score\n",
    "    important_cols = ['post_count', 'avg_score']\n",
    "    important_cols = [col for col in important_cols if col in df.columns]\n",
    "\n",
    "    for col in important_cols:\n",
    "        df[f'{col}_3day_avg'] = df[col].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "    print(f\"Created forecasting features: {df.shape[1]} total columns\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1758451702413,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "8Xa-5_GHTNvC"
   },
   "outputs": [],
   "source": [
    "# preprocessed_datasets = results\n",
    "\n",
    "# # run the topic modeling pipeline\n",
    "# results, modeler = run_topic_modeling_pipeline(preprocessed_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1758451702420,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "jlGP2X4aZiD9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # analyze trends for each crypto\n",
    "# for name, daily_df in results.items():\n",
    "#     if len(daily_df) > 0:\n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"ANALYZING {name.upper()} TRENDS\")\n",
    "#         print(f\"{'='*60}\")\n",
    "\n",
    "#         # Get trend analysis\n",
    "#         analyze_topic_trends(daily_df, modeler)\n",
    "\n",
    "#         # Create forecasting features with lags\n",
    "#         forecasting_df = create_forecasting_features(daily_df)\n",
    "\n",
    "#         # Save if needed\n",
    "#         forecasting_df.to_csv(f'actual_topic_modelled/{name}_lda_forecasting_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1758451702511,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "XKUDb5UnZtlx"
   },
   "outputs": [],
   "source": [
    "eth_lda_forecast_df = pd.read_csv('actual_topic_modelled/eth_lda_forecasting_features.csv')\n",
    "btc_lda_forecast_df = pd.read_csv('actual_topic_modelled/btc_lda_forecasting_features.csv')\n",
    "cryptomarkets_lda_forecast_df = pd.read_csv('actual_topic_modelled/cryptomarkets_lda_forecasting_features.csv')\n",
    "combined_lda_forecast_df = pd.read_csv('actual_topic_modelled/combined_lda_forecasting_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758451702512,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "h1LoFN1_bRDH",
    "outputId": "02e30f86-cb40-4a45-a298-6fdfb8d9b7d3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"topic_summary\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"topic_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"topic_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Topic_1: bch, cash, money\",\n          \"Topic_4: today, bought, bought today\",\n          \"Topic_2: term, long, expect\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top_3_words\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"bch, cash, money\",\n          \"today, bought, bought today\",\n          \"term, long, expect\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "topic_summary"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-0b38eca0-ae6d-4cdd-a389-fb95f3c8a08b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>top_3_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Topic_0: loan, taking, usd</td>\n",
       "      <td>loan, taking, usd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Topic_1: bch, cash, money</td>\n",
       "      <td>bch, cash, money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Topic_2: term, long, expect</td>\n",
       "      <td>term, long, expect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Topic_3: exactly, april, hit</td>\n",
       "      <td>exactly, april, hit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Topic_4: today, bought, bought today</td>\n",
       "      <td>today, bought, bought today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b38eca0-ae6d-4cdd-a389-fb95f3c8a08b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-0b38eca0-ae6d-4cdd-a389-fb95f3c8a08b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-0b38eca0-ae6d-4cdd-a389-fb95f3c8a08b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-34654dad-94b3-4d1c-a935-eb4f08a35a7e\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-34654dad-94b3-4d1c-a935-eb4f08a35a7e')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-34654dad-94b3-4d1c-a935-eb4f08a35a7e button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_7463734e-37a5-42dc-ac0c-f3236d71a133\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('topic_summary')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_7463734e-37a5-42dc-ac0c-f3236d71a133 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('topic_summary');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   topic_id                            topic_name                  top_3_words\n",
       "0         0            Topic_0: loan, taking, usd            loan, taking, usd\n",
       "1         1             Topic_1: bch, cash, money             bch, cash, money\n",
       "2         2           Topic_2: term, long, expect           term, long, expect\n",
       "3         3          Topic_3: exactly, april, hit          exactly, april, hit\n",
       "4         4  Topic_4: today, bought, bought today  today, bought, bought today"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_summary = pd.read_csv('actual_topic_modelled/topic_summary.csv')\n",
    "topic_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1758451702545,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "uUmHqlugaDk_",
    "outputId": "8eb93a1c-e06a-4c95-adca-c0da47502be6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33 entries, 0 to 32\n",
      "Data columns (total 47 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   date                     33 non-null     object \n",
      " 1   subreddit                33 non-null     object \n",
      " 2   post_count               33 non-null     int64  \n",
      " 3   avg_score                33 non-null     float64\n",
      " 4   total_score              33 non-null     float64\n",
      " 5   score_std                33 non-null     float64\n",
      " 6   avg_topic_confidence     33 non-null     float64\n",
      " 7   avg_topic_0              33 non-null     float64\n",
      " 8   std_topic_0              33 non-null     float64\n",
      " 9   max_topic_0              33 non-null     float64\n",
      " 10  avg_topic_1              33 non-null     float64\n",
      " 11  std_topic_1              33 non-null     float64\n",
      " 12  max_topic_1              33 non-null     float64\n",
      " 13  avg_topic_2              33 non-null     float64\n",
      " 14  std_topic_2              33 non-null     float64\n",
      " 15  max_topic_2              33 non-null     float64\n",
      " 16  avg_topic_3              33 non-null     float64\n",
      " 17  std_topic_3              33 non-null     float64\n",
      " 18  max_topic_3              33 non-null     float64\n",
      " 19  avg_topic_4              33 non-null     float64\n",
      " 20  std_topic_4              33 non-null     float64\n",
      " 21  max_topic_4              33 non-null     float64\n",
      " 22  dominant_topic_0_count   33 non-null     int64  \n",
      " 23  dominant_topic_0_pct     33 non-null     float64\n",
      " 24  dominant_topic_1_count   33 non-null     int64  \n",
      " 25  dominant_topic_1_pct     33 non-null     float64\n",
      " 26  dominant_topic_2_count   33 non-null     int64  \n",
      " 27  dominant_topic_2_pct     33 non-null     float64\n",
      " 28  dominant_topic_3_count   33 non-null     int64  \n",
      " 29  dominant_topic_3_pct     33 non-null     float64\n",
      " 30  dominant_topic_4_count   33 non-null     int64  \n",
      " 31  dominant_topic_4_pct     33 non-null     float64\n",
      " 32  topic_diversity_entropy  33 non-null     float64\n",
      " 33  unique_topics_count      33 non-null     int64  \n",
      " 34  topic_concentration      33 non-null     float64\n",
      " 35  post_count_lag_1         32 non-null     float64\n",
      " 36  avg_score_lag_1          32 non-null     float64\n",
      " 37  avg_topic_0_lag_1        32 non-null     float64\n",
      " 38  avg_topic_1_lag_1        32 non-null     float64\n",
      " 39  avg_topic_4_lag_1        32 non-null     float64\n",
      " 40  post_count_lag_3         30 non-null     float64\n",
      " 41  avg_score_lag_3          30 non-null     float64\n",
      " 42  avg_topic_0_lag_3        30 non-null     float64\n",
      " 43  avg_topic_1_lag_3        30 non-null     float64\n",
      " 44  avg_topic_4_lag_3        30 non-null     float64\n",
      " 45  post_count_3day_avg      33 non-null     float64\n",
      " 46  avg_score_3day_avg       33 non-null     float64\n",
      "dtypes: float64(38), int64(7), object(2)\n",
      "memory usage: 12.2+ KB\n"
     ]
    }
   ],
   "source": [
    "btc_lda_forecast_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_kwrjPnXOAA"
   },
   "source": [
    "**BERTopic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1758451702570,
     "user": {
      "displayName": "Mya Thet Chai",
      "userId": "09845320336926044373"
     },
     "user_tz": -480
    },
    "id": "_gwLjYeqexHQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# CLEAN SOLUTION: Just modify your existing CryptoTopicModeler class\n",
    "class BERTopicCryptoModeler:\n",
    "    def __init__(self, n_topics=10, random_state=42):\n",
    "        \"\"\"BERTopic version - same interface as LDA\"\"\"\n",
    "        self.n_topics = n_topics\n",
    "        self.random_state = random_state  # Store for later use\n",
    "        self.topic_model = None\n",
    "        self.topic_names = None\n",
    "\n",
    "    def fit_model(self, texts, optimize_topics=True, max_features=1000):\n",
    "        \"\"\"Fit BERTopic model - same method name for compatibility\"\"\"\n",
    "        try:\n",
    "            from bertopic import BERTopic\n",
    "            print(f\"Fitting BERTopic model on {len(texts):,} documents...\")\n",
    "\n",
    "\n",
    "            self.topic_model = BERTopic(\n",
    "                embedding_model=\"all-MiniLM-L6-v2\",\n",
    "                nr_topics=self.n_topics if not optimize_topics else None,\n",
    "                verbose=True,\n",
    "                calculate_probabilities=True\n",
    "            )\n",
    "\n",
    "            # fit model\n",
    "            topics, probs = self.topic_model.fit_transform(texts)\n",
    "\n",
    "            # get topic info and update n_topics\n",
    "            topic_info = self.topic_model.get_topic_info()\n",
    "            valid_topics = topic_info[topic_info['Topic'] != -1]\n",
    "            self.n_topics = len(valid_topics)\n",
    "\n",
    "            # generate topic names compatible with existing code\n",
    "            self.topic_names = []\n",
    "            for _, row in valid_topics.iterrows():\n",
    "                topic_id = row['Topic']\n",
    "                topic_words = self.topic_model.get_topic(topic_id)\n",
    "                top_3_words = [word for word, _ in topic_words[:3]]\n",
    "                topic_name = f\"Topic_{topic_id}: {', '.join(top_3_words)}\"\n",
    "                self.topic_names.append(topic_name)\n",
    "\n",
    "            print(f\"BERTopic model fitting complete! Found {self.n_topics} topics\")\n",
    "            return np.zeros((len(texts), 100))  # dummy return for compatibility\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"BERTopic not installed. Run: pip install bertopic sentence-transformers\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting BERTopic model: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_topic_details(self, n_words=10):\n",
    "        \"\"\"Get topic details - same format as LDA\"\"\"\n",
    "        if not self.topic_model or not self.topic_names:\n",
    "            return []\n",
    "\n",
    "        topic_details = []\n",
    "        topic_info = self.topic_model.get_topic_info()\n",
    "        valid_topics = topic_info[topic_info['Topic'] != -1]\n",
    "\n",
    "        for i, (_, row) in enumerate(valid_topics.iterrows()):\n",
    "            topic_id = row['Topic']\n",
    "            topic_words = self.topic_model.get_topic(topic_id)\n",
    "            top_words_weights = [(word, score) for word, score in topic_words[:n_words]]\n",
    "\n",
    "            topic_details.append({\n",
    "                'topic_id': i,\n",
    "                'topic_name': self.topic_names[i],\n",
    "                'top_words': top_words_weights\n",
    "            })\n",
    "\n",
    "        return topic_details\n",
    "\n",
    "    def get_document_topics(self, texts):\n",
    "        \"\"\"Get topic probabilities - same format as LDA\"\"\"\n",
    "        if not self.topic_model:\n",
    "            raise ValueError(\"Model not fitted. Call fit_lda_model first.\")\n",
    "\n",
    "        topics, probs = self.topic_model.transform(texts)\n",
    "\n",
    "        # create probability matrix like LDA\n",
    "        n_docs = len(texts)\n",
    "        doc_topic_probs = np.zeros((n_docs, self.n_topics))\n",
    "\n",
    "        # map BERTopic IDs to sequential IDs\n",
    "        topic_info = self.topic_model.get_topic_info()\n",
    "        valid_topics = topic_info[topic_info['Topic'] != -1]['Topic'].tolist()\n",
    "        topic_id_mapping = {bert_id: seq_id for seq_id, bert_id in enumerate(valid_topics)}\n",
    "\n",
    "        for doc_idx, (topic_id, prob_row) in enumerate(zip(topics, probs)):\n",
    "            if topic_id in topic_id_mapping:\n",
    "                seq_id = topic_id_mapping[topic_id]\n",
    "                doc_topic_probs[doc_idx, seq_id] = prob_row.max() if hasattr(prob_row, 'max') else prob_row\n",
    "\n",
    "        return doc_topic_probs\n",
    "\n",
    "    def create_daily_topic_features(self, df):\n",
    "        \"\"\"\n",
    "        COMPLETE METHOD: Create daily topic features - same as LDA version\n",
    "        \"\"\"\n",
    "        print(\"Creating daily topic features...\")\n",
    "\n",
    "        if 'processed_text' not in df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'processed_text' column\")\n",
    "\n",
    "        # get topic probabilities for all documents\n",
    "        topic_probs = self.get_document_topics(df['processed_text'])\n",
    "\n",
    "        # add topic probabilities to dataframe (make a copy to avoid modifying original)\n",
    "        df_copy = df.copy()\n",
    "        topic_columns = [f'topic_{i}' for i in range(self.n_topics)]\n",
    "        for i, col in enumerate(topic_columns):\n",
    "            df_copy[col] = topic_probs[:, i]\n",
    "\n",
    "        # add dominant topic\n",
    "        df_copy['dominant_topic'] = topic_probs.argmax(axis=1)\n",
    "        df_copy['topic_confidence'] = topic_probs.max(axis=1)\n",
    "\n",
    "        # group by date and subreddit for daily aggregation\n",
    "        daily_features_list = []\n",
    "        groupby_cols = ['date', 'subreddit'] if 'subreddit' in df_copy.columns else ['date']\n",
    "\n",
    "        for group_keys, group_df in df_copy.groupby(groupby_cols):\n",
    "            if isinstance(group_keys, tuple):\n",
    "                date, subreddit = group_keys\n",
    "            else:\n",
    "                date, subreddit = group_keys, 'unknown'\n",
    "\n",
    "            if pd.isna(date):\n",
    "                continue\n",
    "\n",
    "            # basic features\n",
    "            features = {\n",
    "                'date': date,\n",
    "                'subreddit': subreddit,\n",
    "                'post_count': len(group_df),\n",
    "                'avg_score': group_df['score'].mean(),\n",
    "                'total_score': group_df['score'].sum(),\n",
    "                'score_std': group_df['score'].std() if len(group_df) > 1 else 0,\n",
    "                'avg_topic_confidence': group_df['topic_confidence'].mean(),\n",
    "            }\n",
    "\n",
    "            # topic probability features\n",
    "            for i, topic_col in enumerate(topic_columns):\n",
    "                features[f'avg_{topic_col}'] = group_df[topic_col].mean()\n",
    "                features[f'std_{topic_col}'] = group_df[topic_col].std() if len(group_df) > 1 else 0\n",
    "                features[f'max_{topic_col}'] = group_df[topic_col].max()\n",
    "\n",
    "            # dominant topic distribution\n",
    "            dominant_counts = group_df['dominant_topic'].value_counts()\n",
    "            for topic_idx in range(self.n_topics):\n",
    "                count = dominant_counts.get(topic_idx, 0)\n",
    "                features[f'dominant_topic_{topic_idx}_count'] = count\n",
    "                features[f'dominant_topic_{topic_idx}_pct'] = count / len(group_df)\n",
    "\n",
    "            # topic diversity metrics\n",
    "            topic_dist = group_df['dominant_topic'].value_counts(normalize=True)\n",
    "            if len(topic_dist) > 1:\n",
    "                entropy = -sum(p * np.log2(p) for p in topic_dist if p > 0)\n",
    "                features['topic_diversity_entropy'] = entropy\n",
    "            else:\n",
    "                features['topic_diversity_entropy'] = 0\n",
    "\n",
    "            features['unique_topics_count'] = group_df['dominant_topic'].nunique()\n",
    "            features['topic_concentration'] = dominant_counts.iloc[0] / len(group_df) if len(dominant_counts) > 0 else 0\n",
    "\n",
    "            daily_features_list.append(features)\n",
    "\n",
    "        # convert to DataFrame\n",
    "        daily_features_df = pd.DataFrame(daily_features_list)\n",
    "        daily_features_df = daily_features_df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "        print(f\"Created daily features: {daily_features_df.shape[0]} days, {daily_features_df.shape[1]} features\")\n",
    "        return daily_features_df\n",
    "\n",
    "\n",
    "\n",
    "def run_bertopic_pipeline(datasets):\n",
    "    \"\"\"Same as existing pipeline but with BERTopic\"\"\"\n",
    "\n",
    "\n",
    "    if not any(len(df) > 0 for df in datasets.values() if isinstance(df, pd.DataFrame)):\n",
    "        print(\"No preprocessed data found.\")\n",
    "        return None, None\n",
    "\n",
    "    if 'combined' in datasets and len(datasets['combined']) > 0:\n",
    "        training_data = datasets['combined']\n",
    "        print(f\"Using combined dataset for topic modeling: {len(training_data):,} posts\")\n",
    "    else:\n",
    "        largest_name = max(\n",
    "            [k for k in datasets.keys() if k != 'combined'],\n",
    "            key=lambda x: len(datasets[x]) if isinstance(datasets[x], pd.DataFrame) else 0\n",
    "        )\n",
    "        training_data = datasets[largest_name]\n",
    "        print(f\"Using {largest_name} dataset for topic modeling: {len(training_data):,} posts\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BERTOPIC MODELING\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    modeler = BERTopicCryptoModeler(n_topics=10, random_state=42)\n",
    "\n",
    "    modeler.fit_model(\n",
    "        training_data['processed_text'].tolist(),\n",
    "        optimize_topics=False,\n",
    "        max_features=1000\n",
    "    )\n",
    "\n",
    "    # display topics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DISCOVERED TOPICS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    topic_details = modeler.get_topic_details()\n",
    "    for topic in topic_details:\n",
    "        print(f\"\\n{topic['topic_name']}\")\n",
    "        top_words = [f\"{word}({weight:.3f})\" for word, weight in topic['top_words'][:5]]\n",
    "        print(f\"  Top words: {', '.join(top_words)}\")\n",
    "\n",
    "    # process each dataset\n",
    "    results = {}\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CREATING DAILY TOPIC FEATURES\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        if len(df) == 0:\n",
    "            print(f\"\\nSkipping {name}: no data\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {name.upper()} dataset...\")\n",
    "\n",
    "        try:\n",
    "            daily_features = modeler.create_daily_topic_features(df)\n",
    "\n",
    "            output_file = f'actual_topic_modelled/{name}_daily_bertopic_features.csv'\n",
    "            daily_features.to_csv(output_file, index=False)\n",
    "            print(f\"Saved to: {output_file}\")\n",
    "\n",
    "            results[name] = daily_features\n",
    "\n",
    "            if len(daily_features) > 0:\n",
    "                print(f\"  Shape: {daily_features.shape}\")\n",
    "                print(f\"  Date range: {daily_features['date'].min()} to {daily_features['date'].max()}\")\n",
    "                print(f\"  Avg posts per day: {daily_features['post_count'].mean():.1f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {name}: {e}\")\n",
    "            results[name] = pd.DataFrame()\n",
    "\n",
    "    return results, modeler\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jIwG73kke7qR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using combined dataset for topic modeling: 30,000 posts\n",
      "\n",
      "============================================================\n",
      "BERTOPIC MODELING\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 10:48:54,408 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting BERTopic model on 30,000 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f8feb341864ac2acf959b315b20d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 10:55:59,258 - BERTopic - Embedding - Completed \n",
      "2025-09-21 10:55:59,260 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-09-21 10:57:29,503 - BERTopic - Dimensionality - Completed \n",
      "2025-09-21 10:57:29,507 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-21 13:12:44,828 - BERTopic - Cluster - Completed \n",
      "2025-09-21 13:12:44,855 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-09-21 13:12:47,602 - BERTopic - Representation - Completed \n",
      "2025-09-21 13:12:47,605 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-09-21 13:12:47,825 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-21 13:12:48,515 - BERTopic - Representation - Completed \n",
      "2025-09-21 13:12:48,522 - BERTopic - Topic reduction - Reduced number of topics from 1064 to 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic model fitting complete! Found 9 topics\n",
      "\n",
      "============================================================\n",
      "DISCOVERED TOPICS\n",
      "============================================================\n",
      "\n",
      "Topic_0: year, money, loan\n",
      "  Top words: year(0.019), money(0.016), loan(0.016), bought(0.016), next(0.014)\n",
      "\n",
      "Topic_1: node, proof, work\n",
      "  Top words: node(0.297), proof(0.203), work(0.191), enable(0.133), axiom(0.130)\n",
      "\n",
      "Topic_2: sub, discussion, ticker\n",
      "  Top words: sub(0.240), discussion(0.161), ticker(0.141), subreddit(0.133), name(0.123)\n",
      "\n",
      "Topic_3: want, two, thing\n",
      "  Top words: want(0.402), two(0.391), thing(0.312), red(0.248), brings(0.234)\n",
      "\n",
      "Topic_4: goog, amzn, meta\n",
      "  Top words: goog(0.386), amzn(0.385), meta(0.364), silver(0.341), bigger(0.302)\n",
      "\n",
      "Topic_5: supply, xcv, total\n",
      "  Top words: supply(0.350), xcv(0.216), total(0.190), genesis(0.187), flow(0.156)\n",
      "\n",
      "Topic_6: que, pero, refuge\n",
      "  Top words: que(0.336), pero(0.142), refuge(0.124), valeur(0.124), una(0.123)\n",
      "\n",
      "Topic_7: announces, agency, chatgpt\n",
      "  Top words: announces(0.741), agency(0.692), chatgpt(0.605), integration(0.573), across(0.506)\n",
      "\n",
      "Topic_8: javascript, function, send\n",
      "  Top words: javascript(1.205), function(0.895), send(0.758), wallet(0.516), acc(0.515)\n",
      "\n",
      "============================================================\n",
      "CREATING DAILY TOPIC FEATURES\n",
      "============================================================\n",
      "\n",
      "Processing ETH dataset...\n",
      "Creating daily topic features...\n",
      "Error processing eth: 1501\n",
      "\n",
      "Processing BTC dataset...\n",
      "Creating daily topic features...\n",
      "Error processing btc: 15044\n",
      "\n",
      "Processing CRYPTOMARKETS dataset...\n",
      "Creating daily topic features...\n",
      "Error processing cryptomarkets: 2731\n",
      "\n",
      "Processing COMBINED dataset...\n",
      "Creating daily topic features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c87024b60b441b991e569e31f615de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 13:19:46,660 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2025-09-21 13:19:46,765 - BERTopic - Dimensionality - Completed \n",
      "2025-09-21 13:19:46,768 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2025-09-21 13:19:48,929 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n"
     ]
    }
   ],
   "source": [
    "results, modeler = run_bertopic_pipeline(preprocessed_datasets)\n",
    "\n",
    "for name, daily_df in results.items():\n",
    "    if len(daily_df) > 0:\n",
    "        analyze_topic_trends(daily_df, modeler)\n",
    "        forecasting_df = create_forecasting_features(daily_df)\n",
    "        forecasting_df.to_csv(f'actual_topic_modelled/{name}_bertopic_forecasting_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVMxr8c-REXW"
   },
   "source": [
    "refactoring attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7wft05ORCeE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class BaseTopicModeler:\n",
    "    \"\"\"\n",
    "    Base class containing all shared functionality for topic modeling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_topics=10, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize base topic modeler\n",
    "\n",
    "        Parameters:\n",
    "        - n_topics: Number of topics to discover\n",
    "        - random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "        self.random_state = random_state\n",
    "        self.topic_names = None\n",
    "\n",
    "    def create_daily_topic_features(self, df):\n",
    "        \"\"\"\n",
    "        Create daily aggregated topic features from preprocessed dataframe\n",
    "        This method is identical for both LDA and BERTopic\n",
    "\n",
    "        Parameters:\n",
    "        - df: Preprocessed dataframe with required columns\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame with daily topic features\n",
    "        \"\"\"\n",
    "        print(\"Creating daily topic features...\")\n",
    "\n",
    "        if 'processed_text' not in df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'processed_text' column\")\n",
    "\n",
    "        # Get topic probabilities for all documents\n",
    "        topic_probs = self.get_document_topics(df['processed_text'])\n",
    "\n",
    "        # Add topic probabilities to dataframe (make a copy)\n",
    "        df_copy = df.copy()\n",
    "        topic_columns = [f'topic_{i}' for i in range(self.n_topics)]\n",
    "        for i, col in enumerate(topic_columns):\n",
    "            df_copy[col] = topic_probs[:, i]\n",
    "\n",
    "        # Add dominant topic and confidence\n",
    "        df_copy['dominant_topic'] = topic_probs.argmax(axis=1)\n",
    "        df_copy['topic_confidence'] = topic_probs.max(axis=1)\n",
    "\n",
    "        # Group by date and subreddit for daily aggregation\n",
    "        daily_features_list = []\n",
    "        groupby_cols = ['date', 'subreddit'] if 'subreddit' in df_copy.columns else ['date']\n",
    "\n",
    "        for group_keys, group_df in df_copy.groupby(groupby_cols):\n",
    "            if isinstance(group_keys, tuple):\n",
    "                date, subreddit = group_keys\n",
    "            else:\n",
    "                date, subreddit = group_keys, 'unknown'\n",
    "\n",
    "            if pd.isna(date):\n",
    "                continue\n",
    "\n",
    "            # Basic features\n",
    "            features = {\n",
    "                'date': date,\n",
    "                'subreddit': subreddit,\n",
    "                'post_count': len(group_df),\n",
    "                'avg_score': group_df['score'].mean(),\n",
    "                'total_score': group_df['score'].sum(),\n",
    "                'score_std': group_df['score'].std() if len(group_df) > 1 else 0,\n",
    "                'avg_topic_confidence': group_df['topic_confidence'].mean(),\n",
    "            }\n",
    "\n",
    "            # Topic probability features\n",
    "            for i, topic_col in enumerate(topic_columns):\n",
    "                features[f'avg_{topic_col}'] = group_df[topic_col].mean()\n",
    "                features[f'std_{topic_col}'] = group_df[topic_col].std() if len(group_df) > 1 else 0\n",
    "                features[f'max_{topic_col}'] = group_df[topic_col].max()\n",
    "\n",
    "            # Dominant topic distribution\n",
    "            dominant_counts = group_df['dominant_topic'].value_counts()\n",
    "            for topic_idx in range(self.n_topics):\n",
    "                count = dominant_counts.get(topic_idx, 0)\n",
    "                features[f'dominant_topic_{topic_idx}_count'] = count\n",
    "                features[f'dominant_topic_{topic_idx}_pct'] = count / len(group_df)\n",
    "\n",
    "            # Topic diversity metrics\n",
    "            topic_dist = group_df['dominant_topic'].value_counts(normalize=True)\n",
    "            if len(topic_dist) > 1:\n",
    "                entropy = -sum(p * np.log2(p) for p in topic_dist if p > 0)\n",
    "                features['topic_diversity_entropy'] = entropy\n",
    "            else:\n",
    "                features['topic_diversity_entropy'] = 0\n",
    "\n",
    "            features['unique_topics_count'] = group_df['dominant_topic'].nunique()\n",
    "            features['topic_concentration'] = dominant_counts.iloc[0] / len(group_df) if len(dominant_counts) > 0 else 0\n",
    "\n",
    "            daily_features_list.append(features)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        daily_features_df = pd.DataFrame(daily_features_list)\n",
    "        daily_features_df = daily_features_df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "        print(f\"Created daily features: {daily_features_df.shape[0]} days, {daily_features_df.shape[1]} features\")\n",
    "        return daily_features_df\n",
    "\n",
    "    # Abstract methods that must be implemented by subclasses\n",
    "    def fit_model(self, texts, **kwargs):\n",
    "        \"\"\"Must be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement fit_model\")\n",
    "\n",
    "    def get_topic_details(self, n_words=10):\n",
    "        \"\"\"Must be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement get_topic_details\")\n",
    "\n",
    "    def get_document_topics(self, texts):\n",
    "        \"\"\"Must be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement get_document_topics\")\n",
    "\n",
    "\n",
    "class LDATopicModeler(BaseTopicModeler):\n",
    "    \"\"\"\n",
    "    LDA-specific topic modeling implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_topics=10, random_state=42):\n",
    "        super().__init__(n_topics, random_state)\n",
    "        self.lda_model = None\n",
    "        self.vectorizer = None\n",
    "        self.feature_names = None\n",
    "\n",
    "    def fit_model(self, texts, optimize_topics=True, max_features=1000):\n",
    "        \"\"\"Fit LDA model on preprocessed texts\"\"\"\n",
    "        print(f\"Fitting LDA model on {len(texts):,} documents...\")\n",
    "\n",
    "        # Create TF-IDF vectorizer\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            min_df=2,\n",
    "            max_df=0.8,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english'\n",
    "        )\n",
    "\n",
    "        # Fit and transform texts\n",
    "        print(\"Creating document-term matrix...\")\n",
    "        doc_term_matrix = self.vectorizer.fit_transform(texts)\n",
    "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "        print(f\"Document-term matrix shape: {doc_term_matrix.shape}\")\n",
    "\n",
    "        # Optimize number of topics if requested\n",
    "        if optimize_topics and len(texts) > 100:\n",
    "            print(\"Optimizing number of topics...\")\n",
    "            self.n_topics = self._optimize_topic_number(doc_term_matrix)\n",
    "\n",
    "        # Fit final LDA model\n",
    "        print(f\"Fitting LDA with {self.n_topics} topics...\")\n",
    "        self.lda_model = LatentDirichletAllocation(\n",
    "            n_components=self.n_topics,\n",
    "            random_state=self.random_state,\n",
    "            max_iter=20,\n",
    "            learning_method='batch'\n",
    "        )\n",
    "\n",
    "        self.lda_model.fit(doc_term_matrix)\n",
    "        self.topic_names = self._generate_topic_names()\n",
    "\n",
    "        print(\"LDA model fitting complete!\")\n",
    "        return doc_term_matrix\n",
    "\n",
    "    def _optimize_topic_number(self, doc_term_matrix):\n",
    "        \"\"\"Optimize number of topics using grid search\"\"\"\n",
    "        topic_range = [5, 8, 10, 12, 15]\n",
    "        if doc_term_matrix.shape[0] > 20000:\n",
    "            topic_range = [5, 8, 10]\n",
    "\n",
    "        print(f\"Testing topic numbers: {topic_range}\")\n",
    "\n",
    "        param_grid = {'n_components': topic_range}\n",
    "        lda = LatentDirichletAllocation(\n",
    "            random_state=self.random_state,\n",
    "            max_iter=10,\n",
    "            learning_method='batch'\n",
    "        )\n",
    "\n",
    "        cv_folds = min(3, max(2, len(topic_range)))\n",
    "        grid_search = GridSearchCV(\n",
    "            lda, param_grid, cv=cv_folds,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1, verbose=0\n",
    "        )\n",
    "\n",
    "        grid_search.fit(doc_term_matrix)\n",
    "        optimal_topics = grid_search.best_params_['n_components']\n",
    "\n",
    "        print(f\"Optimal number of topics: {optimal_topics}\")\n",
    "        return optimal_topics\n",
    "\n",
    "    def _generate_topic_names(self, n_words=3):\n",
    "        \"\"\"Generate interpretable topic names\"\"\"\n",
    "        if not self.lda_model or self.feature_names is None:\n",
    "            return []\n",
    "\n",
    "        topic_names = []\n",
    "        for topic_idx, topic in enumerate(self.lda_model.components_):\n",
    "            top_word_indices = topic.argsort()[-n_words:][::-1]\n",
    "            top_words = [self.feature_names[i] for i in top_word_indices]\n",
    "            topic_name = f\"Topic_{topic_idx}: {', '.join(top_words)}\"\n",
    "            topic_names.append(topic_name)\n",
    "\n",
    "        return topic_names\n",
    "\n",
    "    def get_topic_details(self, n_words=10):\n",
    "        \"\"\"Get detailed information about each topic\"\"\"\n",
    "        if not self.lda_model or self.feature_names is None:\n",
    "            return []\n",
    "\n",
    "        topic_details = []\n",
    "        for topic_idx, topic in enumerate(self.lda_model.components_):\n",
    "            top_word_indices = topic.argsort()[-n_words:][::-1]\n",
    "            top_words_weights = [\n",
    "                (self.feature_names[i], topic[i])\n",
    "                for i in top_word_indices\n",
    "            ]\n",
    "\n",
    "            topic_details.append({\n",
    "                'topic_id': topic_idx,\n",
    "                'topic_name': self.topic_names[topic_idx],\n",
    "                'top_words': top_words_weights\n",
    "            })\n",
    "\n",
    "        return topic_details\n",
    "\n",
    "    def get_document_topics(self, texts):\n",
    "        \"\"\"Get topic probabilities for documents\"\"\"\n",
    "        if not self.lda_model or not self.vectorizer:\n",
    "            raise ValueError(\"Model not fitted. Call fit_model first.\")\n",
    "\n",
    "        doc_term_matrix = self.vectorizer.transform(texts)\n",
    "        doc_topic_probs = self.lda_model.transform(doc_term_matrix)\n",
    "        return doc_topic_probs\n",
    "\n",
    "\n",
    "class BERTopicModeler(BaseTopicModeler):\n",
    "    \"\"\"\n",
    "    BERTopic-specific topic modeling implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_topics=10, random_state=42):\n",
    "        super().__init__(n_topics, random_state)\n",
    "        self.topic_model = None\n",
    "\n",
    "    def fit_model(self, texts, optimize_topics=True, max_features=1000):\n",
    "        \"\"\"Fit BERTopic model on preprocessed texts\"\"\"\n",
    "        try:\n",
    "            from bertopic import BERTopic\n",
    "            print(f\"Fitting BERTopic model on {len(texts):,} documents...\")\n",
    "\n",
    "            # Initialize BERTopic model\n",
    "            self.topic_model = BERTopic(\n",
    "                embedding_model=\"all-MiniLM-L6-v2\",\n",
    "                nr_topics=self.n_topics if not optimize_topics else None,\n",
    "                verbose=True,\n",
    "                calculate_probabilities=True\n",
    "            )\n",
    "\n",
    "            # Fit model\n",
    "            topics, probs = self.topic_model.fit_transform(texts)\n",
    "\n",
    "            # Get topic info and update n_topics\n",
    "            topic_info = self.topic_model.get_topic_info()\n",
    "            valid_topics = topic_info[topic_info['Topic'] != -1]\n",
    "            self.n_topics = len(valid_topics)\n",
    "\n",
    "            # Generate topic names\n",
    "            self.topic_names = self._generate_topic_names(valid_topics)\n",
    "\n",
    "            print(f\"BERTopic model fitting complete! Found {self.n_topics} topics\")\n",
    "            return np.zeros((len(texts), 100))  # Dummy return for compatibility\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"BERTopic not installed. Run: pip install bertopic sentence-transformers\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting BERTopic model: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _generate_topic_names(self, valid_topics):\n",
    "        \"\"\"Generate topic names for BERTopic\"\"\"\n",
    "        topic_names = []\n",
    "        for _, row in valid_topics.iterrows():\n",
    "            topic_id = row['Topic']\n",
    "            topic_words = self.topic_model.get_topic(topic_id)\n",
    "            top_3_words = [word for word, _ in topic_words[:3]]\n",
    "            topic_name = f\"Topic_{topic_id}: {', '.join(top_3_words)}\"\n",
    "            topic_names.append(topic_name)\n",
    "        return topic_names\n",
    "\n",
    "    def get_topic_details(self, n_words=10):\n",
    "        \"\"\"Get detailed information about each topic\"\"\"\n",
    "        if not self.topic_model or not self.topic_names:\n",
    "            return []\n",
    "\n",
    "        topic_details = []\n",
    "        topic_info = self.topic_model.get_topic_info()\n",
    "        valid_topics = topic_info[topic_info['Topic'] != -1]\n",
    "\n",
    "        for i, (_, row) in enumerate(valid_topics.iterrows()):\n",
    "            topic_id = row['Topic']\n",
    "            topic_words = self.topic_model.get_topic(topic_id)\n",
    "            top_words_weights = [(word, score) for word, score in topic_words[:n_words]]\n",
    "\n",
    "            topic_details.append({\n",
    "                'topic_id': i,\n",
    "                'topic_name': self.topic_names[i],\n",
    "                'top_words': top_words_weights\n",
    "            })\n",
    "\n",
    "        return topic_details\n",
    "\n",
    "    def get_document_topics(self, texts):\n",
    "        \"\"\"Get topic probabilities for documents\"\"\"\n",
    "        if not self.topic_model:\n",
    "            raise ValueError(\"Model not fitted. Call fit_model first.\")\n",
    "\n",
    "        topics, probs = self.topic_model.transform(texts)\n",
    "\n",
    "        # Create probability matrix like LDA\n",
    "        n_docs = len(texts)\n",
    "        doc_topic_probs = np.zeros((n_docs, self.n_topics))\n",
    "\n",
    "        # Map BERTopic IDs to sequential IDs\n",
    "        topic_info = self.topic_model.get_topic_info()\n",
    "        valid_topics = topic_info[topic_info['Topic'] != -1]['Topic'].tolist()\n",
    "        topic_id_mapping = {bert_id: seq_id for seq_id, bert_id in enumerate(valid_topics)}\n",
    "\n",
    "        for doc_idx, (topic_id, prob_row) in enumerate(zip(topics, probs)):\n",
    "            if topic_id in topic_id_mapping:\n",
    "                seq_id = topic_id_mapping[topic_id]\n",
    "                doc_topic_probs[doc_idx, seq_id] = prob_row.max() if hasattr(prob_row, 'max') else prob_row\n",
    "\n",
    "        return doc_topic_probs\n",
    "\n",
    "\n",
    "def run_topic_modeling_pipeline(datasets, modeler_class=LDATopicModeler):\n",
    "    \"\"\"\n",
    "    Generic pipeline that works with any topic modeler\n",
    "\n",
    "    Parameters:\n",
    "    - datasets: Dictionary of preprocessed DataFrames\n",
    "    - modeler_class: LDATopicModeler or BERTopicModeler class\n",
    "    \"\"\"\n",
    "    # Check if we have data\n",
    "    if not any(len(df) > 0 for df in datasets.values() if isinstance(df, pd.DataFrame)):\n",
    "        print(\"No preprocessed data found.\")\n",
    "        return None, None\n",
    "\n",
    "    # Select training data\n",
    "    if 'combined' in datasets and len(datasets['combined']) > 0:\n",
    "        training_data = datasets['combined']\n",
    "        print(f\"Using combined dataset for topic modeling: {len(training_data):,} posts\")\n",
    "    else:\n",
    "        largest_name = max(\n",
    "            [k for k in datasets.keys() if k != 'combined'],\n",
    "            key=lambda x: len(datasets[x]) if isinstance(datasets[x], pd.DataFrame) else 0\n",
    "        )\n",
    "        training_data = datasets[largest_name]\n",
    "        print(f\"Using {largest_name} dataset for topic modeling: {len(training_data):,} posts\")\n",
    "\n",
    "    # Initialize modeler\n",
    "    algorithm_name = modeler_class.__name__.replace('TopicModeler', '').replace('Modeler', '')\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{algorithm_name.upper()} TOPIC MODELING\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    modeler = modeler_class(n_topics=10, random_state=42)\n",
    "\n",
    "    # Fit model\n",
    "    modeler.fit_model(\n",
    "        training_data['processed_text'].tolist(),\n",
    "        optimize_topics=True,\n",
    "        max_features=1000\n",
    "    )\n",
    "\n",
    "    # Display topics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DISCOVERED TOPICS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    topic_details = modeler.get_topic_details()\n",
    "    for topic in topic_details:\n",
    "        print(f\"\\n{topic['topic_name']}\")\n",
    "        top_words = [f\"{word}({weight:.3f})\" for word, weight in topic['top_words'][:5]]\n",
    "        print(f\"  Top words: {', '.join(top_words)}\")\n",
    "\n",
    "    # Process each dataset\n",
    "    results = {}\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CREATING DAILY TOPIC FEATURES\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        if len(df) == 0:\n",
    "            print(f\"\\nSkipping {name}: no data\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {name.upper()} dataset...\")\n",
    "\n",
    "        try:\n",
    "            daily_features = modeler.create_daily_topic_features(df)\n",
    "\n",
    "            # Save with algorithm-specific filename\n",
    "            algo_suffix = 'lda' if isinstance(modeler, LDATopicModeler) else 'bertopic'\n",
    "            output_file = f'actual_topic_modelled/{name}_daily_{algo_suffix}_features.csv'\n",
    "            daily_features.to_csv(output_file, index=False)\n",
    "            print(f\"Saved to: {output_file}\")\n",
    "\n",
    "            results[name] = daily_features\n",
    "\n",
    "            if len(daily_features) > 0:\n",
    "                print(f\"  Shape: {daily_features.shape}\")\n",
    "                print(f\"  Date range: {daily_features['date'].min()} to {daily_features['date'].max()}\")\n",
    "                print(f\"  Avg posts per day: {daily_features['post_count'].mean():.1f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {name}: {e}\")\n",
    "            results[name] = pd.DataFrame()\n",
    "\n",
    "    return results, modeler\n",
    "\n",
    "\n",
    "# Convenience functions\n",
    "def run_lda_pipeline(datasets):\n",
    "    \"\"\"Run LDA topic modeling pipeline\"\"\"\n",
    "    return run_topic_modeling_pipeline(datasets, LDATopicModeler)\n",
    "\n",
    "\n",
    "def run_bertopic_pipeline(datasets):\n",
    "    \"\"\"Run BERTopic modeling pipeline\"\"\"\n",
    "    return run_topic_modeling_pipeline(datasets, BERTopicModeler)\n",
    "\n",
    "\n",
    "# Usage examples:\n",
    "\"\"\"\n",
    "# Use LDA\n",
    "results_lda, modeler_lda = run_lda_pipeline(datasets)\n",
    "\n",
    "# Use BERTopic\n",
    "results_bert, modeler_bert = run_bertopic_pipeline(datasets)\n",
    "\n",
    "# Or use the generic pipeline directly\n",
    "results, modeler = run_topic_modeling_pipeline(datasets, BERTopicModeler)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "149bf87514864dad9c82e86013e8eb76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c866d60238548aab04341cb4d8e406f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1dcc69beeff74777b83f4ffa610cb736": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "356c4671dc4c418bb2a68b77260d92ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b2982d066eb47eeb89914b20b9ab2c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40f196afe69641609903ca9653c1925e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "554110a606c1472187ef10dd5dd756b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c866d60238548aab04341cb4d8e406f",
      "max": 938,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c62e779c8a074180aa7c0e2462bd4008",
      "value": 938
     }
    },
    "5a5fefd3f5884bed97a1cdad03fc0d9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5eb26091abcc42c5b1605e6ef44a4466": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7006501ef0414e10bdbdfb192917bcc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_149bf87514864dad9c82e86013e8eb76",
      "placeholder": "",
      "style": "IPY_MODEL_bfa6044c00ce43e38049401fbdd1a5db",
      "value": "938/938[07:02&lt;00:00,8.42it/s]"
     }
    },
    "7820bce06c624a129b494be95041fbfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83812054d1a447d086345bf47228f500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2a5351dadf84e97b319838714f64f58",
      "placeholder": "",
      "style": "IPY_MODEL_5a5fefd3f5884bed97a1cdad03fc0d9b",
      "value": "Batches:100%"
     }
    },
    "890fc5dec8c7479b9a7e9360a7dfb392": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b198e18f0a44c29b7bc16fc96df79d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b2982d066eb47eeb89914b20b9ab2c1",
      "placeholder": "",
      "style": "IPY_MODEL_1dcc69beeff74777b83f4ffa610cb736",
      "value": "938/938[06:56&lt;00:00,5.87it/s]"
     }
    },
    "a6f8feb341864ac2acf959b315b20d2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac00fbf80d614be1a3f08f6d721a9313",
       "IPY_MODEL_d6db493a62e84fb1b72532c462fbcf42",
       "IPY_MODEL_7006501ef0414e10bdbdfb192917bcc4"
      ],
      "layout": "IPY_MODEL_356c4671dc4c418bb2a68b77260d92ca"
     }
    },
    "ac00fbf80d614be1a3f08f6d721a9313": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40f196afe69641609903ca9653c1925e",
      "placeholder": "",
      "style": "IPY_MODEL_5eb26091abcc42c5b1605e6ef44a4466",
      "value": "Batches:100%"
     }
    },
    "b2a5351dadf84e97b319838714f64f58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfa6044c00ce43e38049401fbdd1a5db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c62e779c8a074180aa7c0e2462bd4008": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d6db493a62e84fb1b72532c462fbcf42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_890fc5dec8c7479b9a7e9360a7dfb392",
      "max": 938,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7820bce06c624a129b494be95041fbfa",
      "value": 938
     }
    },
    "e9af0f0d50a145fb918ca87378dad21d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9c87024b60b441b991e569e31f615de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_83812054d1a447d086345bf47228f500",
       "IPY_MODEL_554110a606c1472187ef10dd5dd756b4",
       "IPY_MODEL_9b198e18f0a44c29b7bc16fc96df79d6"
      ],
      "layout": "IPY_MODEL_e9af0f0d50a145fb918ca87378dad21d"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
