{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14036808,"sourceType":"datasetVersion","datasetId":8937886}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \"protobuf<4.25.0\" --force-reinstall --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport json\nimport urllib.request\nfrom scipy.special import softmax\nimport os\nimport re\nimport glob\n\n# --- CONFIGURATION ---\n# UPDATE THIS PATH to match where your zip file was unpacked in Kaggle\n# usually: /kaggle/input/your-dataset-name/\nINPUT_ROOT_DIR = '/kaggle/input/eth-reddit-posts-hourly/eth_reddit_daily_batches' \n\n# This is where we save the batch files\nOUTPUT_DIR = '/kaggle/working/batch_results'\n# ---------------------\n\n# Create output directory\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\n# 1. Load Model (Standard)\ndef load_model_from_hgf(model_name: str):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n    config_url = f\"https://huggingface.co/{model_name}/raw/main/config.json\"\n    with urllib.request.urlopen(config_url) as url:\n        config_data = json.loads(url.read().decode())\n    label_mapping = config_data[\"id2label\"]\n    labels = [label_mapping[str(i)] for i in range(len(label_mapping))]\n    return tokenizer, model, labels\n\n# 2. Preprocessing & Sentiment Functions (Standard)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\ndef get_sentiment_scores(text, tokenizer, model, labels) -> dict:\n    res = {}\n    text = preprocess(text)\n    encoded_input = tokenizer(text, return_tensors='pt', truncation=True)\n    output = model(**encoded_input)\n    scores = output[0][0].detach().numpy()\n    scores = softmax(scores)\n    ranking = np.argsort(scores)[::-1]\n    for i in range(scores.shape[0]):\n        l = labels[ranking[i]]\n        s = scores[ranking[i]]\n        res[l] = np.round(float(s), 4)\n    return res\n\ndef process_data_and_aggregate_sentiment(table: pd.DataFrame, tokenizer, model, labels):\n    data = table.copy(deep=True)\n    # Check if empty (some files might be empty after cleaning)\n    if data.empty:\n        return None\n        \n    data['sentiment_dict_without_prefix'] = data['selftext'].astype(str).apply(lambda x: get_sentiment_scores(x, tokenizer, model, labels))\n    data['sentiment_dict_with_prefix'] = data['selftext'].astype(str).apply(lambda x: get_sentiment_scores(\"BTC price outlook: \" + x, tokenizer, model, labels))\n    \n    data['neutral'] = data['sentiment_dict_without_prefix'].apply(lambda x: x['neutral'])\n    data['positive'] = data['sentiment_dict_without_prefix'].apply(lambda x: x['positive'])\n    data['negative'] = data['sentiment_dict_without_prefix'].apply(lambda x: x['negative'])\n    \n    data['neutral_prefix'] = data['sentiment_dict_with_prefix'].apply(lambda x: x['neutral'])\n    data['positive_prefix'] = data['sentiment_dict_with_prefix'].apply(lambda x: x['positive'])\n    data['negative_prefix'] = data['sentiment_dict_with_prefix'].apply(lambda x: x['negative'])\n    \n    data['score'] = data['score'].apply(lambda x: x if x > 0 else 1)\n    data['weighted_positive'] = data['score'] * data['positive']\n    data['weighted_positive_prefix'] = data['score'] * data['positive_prefix']\n\n    b_b1 = data[\"positive\"].sum()/data[\"positive\"].size\n    b_b1_prefix = data[\"positive_prefix\"].sum()/data[\"positive_prefix\"].size\n    b_b2 = data[\"weighted_positive\"].sum()/data['score'].sum()\n    b_b2_prefix = data[\"weighted_positive_prefix\"].sum()/data['score'].sum()\n    b_a4 = data[data[\"positive\"] > 0.8].shape[0]/data.shape[0]\n    b_a4_prefix = data[data[\"positive_prefix\"] > 0.8].shape[0]/data.shape[0]\n    \n    return b_b1, b_b1_prefix, b_b2, b_b2_prefix, b_a4, b_a4_prefix\n\n# --- MAIN EXECUTION ---\n\n# Load model once\nprint(\"Loading Model...\")\ntokenizer, model, labels = load_model_from_hgf(\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n\n# Get list of DATE FOLDERS (Batches)\n# We assume the structure is INPUT_ROOT_DIR / YYYY-MM-DD / files...\ndate_folders = sorted([f.path for f in os.scandir(INPUT_ROOT_DIR) if f.is_dir()])\n\nprint(f\"Found {len(date_folders)} date batches to process.\")\n\nfor folder_path in date_folders:\n    date_name = os.path.basename(folder_path) # e.g., \"2025-12-07\"\n    \n    # 1. DEFINE BATCH OUTPUT FILENAME\n    batch_output_file = os.path.join(OUTPUT_DIR, f\"results_{date_name}.csv\")\n    \n    # 2. CHECK IF ALREADY DONE (Resume Capability)\n    if os.path.exists(batch_output_file):\n        print(f\"Batch {date_name} already exists. Skipping...\")\n        continue\n        \n    print(f\"Processing Batch: {date_name}\")\n    \n    batch_results = [] # Use list for speed\n    \n    # Get all csv files in this specific date folder\n    csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n    \n    for filepath in csv_files:\n        filename = os.path.basename(filepath)\n        \n        # Extract Time from filename (e.g. ..._1129.csv)\n        # We already know the Date from the folder name\n        time_match = re.search(r'_(\\d{4})\\.csv', filename)\n        time_part = time_match.group(1) if time_match else \"Unknown\"\n        \n        try:\n            df = pd.read_csv(filepath)\n            \n            # Process\n            metrics = process_data_and_aggregate_sentiment(df, tokenizer, model, labels)\n            \n            if metrics: # Ensure we didn't get None\n                b_b1, b_b1_prefix, b_b2, b_b2_prefix, b_a4, b_a4_prefix = metrics\n                \n                batch_results.append({\n                    'date': date_name,\n                    'time': time_part,\n                    'file': filename,\n                    'b_b1': b_b1,\n                    'b_b1_prefix': b_b1_prefix,\n                    'b_b2': b_b2,\n                    'b_b2_prefix': b_b2_prefix,\n                    'b_a4': b_a4,\n                    'b_a4_prefix': b_a4_prefix\n                })\n                \n        except Exception as e:\n            print(f\"Error in {filename}: {e}\")\n            \n    # 3. SAVE THIS BATCH IMMEDIATELY\n    if batch_results:\n        batch_df = pd.DataFrame(batch_results)\n        batch_df = batch_df.sort_values(by='time')\n        batch_df.to_csv(batch_output_file, index=False)\n        print(f\"Saved {len(batch_results)} records to {batch_output_file}\")\n    else:\n        print(f\"No valid results for {date_name}\")\n\nprint(\"All batches processed.\")\n\n# --- FINAL MERGE ---\n# Combine all the batch files into one final CSV for convenience\nprint(\"Merging all batches...\")\nall_files = glob.glob(os.path.join(OUTPUT_DIR, \"results_*.csv\"))\nif all_files:\n    combined_df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n    combined_df = combined_df.sort_values(by=['date', 'time'])\n    combined_df.to_csv(\"eth_reddit_sentiments_FINAL.csv\", index=False)\n    print(\"Final merged file saved: eth_reddit_sentiments_FINAL.csv\")\nelse:\n    print(\"No output files found to merge.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}